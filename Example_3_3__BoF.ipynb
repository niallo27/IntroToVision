{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_3_3 _BoF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZ+M5/ZXplTCHbYjrwK/z7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/IntroToVision/blob/main/Example_3_3__BoF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-nEXis_gaV"
      },
      "source": [
        "# Image Classification with Bag of Features \n",
        "In this example we will perform image recognition using the bag of features representation. For this example we will use [Intel Scene Classification Dataset](https://www.kaggle.com/puneet6060/intel-image-classification), which has 6 classes as shown. \n",
        "\n",
        "In addition to loading and preparing the dataset, this example notebook demonstrates the key steps in training and testing a Bag of Features based classifier:\n",
        "\n",
        "\n",
        "*   Building (& Visualising) the Vocabulary\n",
        "*   Converting the Data to the BoF (Histogram Representation)\n",
        "*   Training & testing the classifier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![dataset](https://cdn-images-1.medium.com/max/800/1*Y1y83HN7oI98EJT3LQgv-g.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpZVWnGwt6w0"
      },
      "source": [
        "**HouseKeeping**: Downgrade OpenCV version to allow access to SIFT detector/descriptor, Import Python Packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_QRj1rdyHqa"
      },
      "source": [
        "! yes | pip3 uninstall opencv-python\n",
        "\n",
        "! yes | pip3 uninstall opencv-contrib-python\n",
        "\n",
        "! yes | pip3 install opencv-python==3.4.2.16\n",
        " \n",
        "! yes | pip3 install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69GNKkotqa3n"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from skimage.data import lfw_subset\n",
        "from skimage import feature\n",
        "from skimage import exposure\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn import preprocessing\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XQ_j7f8l0X0"
      },
      "source": [
        "# A. (Part 1) Load Dataset.\n",
        "The Intel Scenes dataset is loaded from [Kaggle](https://www.kaggle.com/puneet6060/intel-image-classification), this allows fast importation of the dataset. Ensure that you have a valid [kaggle.json](https://github.com/tonyscan6003/CE6003/blob/master/images/CE6003_kaggle_data_instructions.pdf) file ready to upload.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAQhFJqLXsOJ"
      },
      "source": [
        "! pip install -q kaggle==1.5.6\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8J3UIaji9zd"
      },
      "source": [
        "#Choose the kaggle.json file that you downloaded\n",
        "! mkdir ~/.kaggle\n",
        "#Make directory named kaggle and copy kaggle.json file there.\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "#Change the permissions of the file.\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Load dataset\n",
        "! kaggle datasets download puneet6060/intel-image-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LobhQsagjAJB"
      },
      "source": [
        "!unzip intel-image-classification.zip > /dev/null\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj5Y_PcvuYyM"
      },
      "source": [
        "# A (part 2). Prepare Dataset:\n",
        "In order to prepare the Intel scenes dataset for use with the BoF approach we need to follow a number of steps:\n",
        "\n",
        "\n",
        "1.   Obtain a list of downloaded files in the test and training folders, we use the glob package to do this.\n",
        "2.   Secondly the files must be imported and added to arrays of images.\n",
        "\n",
        "\n",
        "> *   The images files are selected at random to breakup the directory structure which groups the files by class. \n",
        "* The images are converted to greyscale.\n",
        "* The images are scaled so that they are all the same size.\n",
        "\n",
        "3. The labels for each image are obtained from the file string (in step 1). In order to use the SKlearn classifiers (Part D), the labels must be numerical. The sklearn preprocessing methods can be used to convert the label to a numerical value.    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUxfWmrivxAU"
      },
      "source": [
        "1. Create File List for Training & Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjQfMZNlXurx"
      },
      "source": [
        "def gen_list():\n",
        "    #use glob command to find the appropiate natural image files.  \n",
        "    train_img_list = glob.glob('/content/seg_train/seg_train/*/*.jpg')\n",
        "    #Obtain Class labels from directory name\n",
        "    train_label_list = [train_img_list[i].split('/')[4]  for i in range(0,len(train_img_list))]\n",
        " \n",
        "    test_img_list = glob.glob('/content/seg_test/seg_test/*/*.jpg')\n",
        "    test_label_list = [test_img_list[i].split('/')[4]  for i in range(0,len(test_img_list))]\n",
        "    return train_img_list,train_label_list,test_img_list,test_label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NklsQ1PSdrs_"
      },
      "source": [
        "train_img_list,train_label_list,test_img_list,test_label_list = gen_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlUpqMEQBVDn"
      },
      "source": [
        "max_train_examples = len(train_img_list)\n",
        "max_test_examples = len(test_img_list)\n",
        "print('The maximum number available of Training images are ', max_train_examples)\n",
        "print('The maximum number available of Test images are ', max_test_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3cXaMxvGUfv"
      },
      "source": [
        "2. Import (greyscale) image files into array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LPqjoQyAdk"
      },
      "source": [
        "# Target Image size\n",
        "HW_trg = 150\n",
        "\n",
        "# Generator function to yield random order image file each time generator is called\n",
        "def _sel_img_rnd():   \n",
        "    shuff_all_list = np.arange(max_train_examples)\n",
        "    np.random.shuffle(shuff_all_list)\n",
        "    fid_list = shuff_all_list[0:num_train_examples]\n",
        "    i =0\n",
        "    while 1:\n",
        "        fid = fid_list[i] \n",
        "        img_str = train_img_list[fid] \n",
        "        label_str = train_label_list[fid] \n",
        "        i += 1\n",
        "        yield img_str,label_str\n",
        "\n",
        "# Resize short axis to n and crop the image to n x n\n",
        "def _resize_crop(image):\n",
        "  img_H = np.shape(image)[0]\n",
        "  img_W = np.shape(image)[1]\n",
        "  if (img_H == HW_trg) and  (img_W == HW_trg):\n",
        "     crop_image = image\n",
        "  else:   \n",
        "     flag_1 = img_H>img_W\n",
        "     min_dim_size = min(img_H,img_W) \n",
        "     scale_fac = HW_trg/min_dim_size\n",
        "     scale_img_H = int(np.ceil(np.shape(image)[0]*scale_fac))\n",
        "     scale_img_W = int(np.ceil(np.shape(image)[1]*scale_fac))\n",
        "     rescale_image = cv2.resize(image, (scale_img_W,scale_img_H), interpolation = cv2.INTER_AREA)\n",
        "     rescale_H = np.shape(rescale_image)[0]\n",
        "     rescale_W = np.shape(rescale_image)[1]\n",
        "     if flag_1 == True:                  # Height greater than width\n",
        "        c_y = int(rescale_H//2)\n",
        "        c_y_min = int(c_y-HW_trg//2)\n",
        "        c_y_max = int(c_y+HW_trg//2)\n",
        "        crop_image = rescale_image[c_y_min:c_y_max, 0:HW_trg]\n",
        "     else:                              # Width greater than height\n",
        "        c_x = rescale_W//2 \n",
        "        c_x_min = int(c_x-HW_trg//2)\n",
        "        c_x_max = int(c_x+HW_trg//2)\n",
        "        crop_image = rescale_image[0:HW_trg, c_x_min:c_x_max]     \n",
        "  return crop_image\n",
        "\n",
        "# Function to read image from string\n",
        "def _read_image(image_str,label_str):\n",
        "    real_image = cv2.imread(image_str, cv2.IMREAD_GRAYSCALE)\n",
        "    return real_image,label_str\n",
        "\n",
        "# Main function to create array of images\n",
        "def gen_img_stack(n_examples,rnd=True): \n",
        "    X_data = np.empty([HW_trg,HW_trg])\n",
        "    y_train = []\n",
        "    gen = _sel_img_rnd()\n",
        "    for i in tqdm(range(n_examples)):\n",
        "       # File name and label from generator       \n",
        "       image_str,label_str = next(gen)\n",
        "       image,label_str = _read_image(image_str,label_str)\n",
        "       image = _resize_crop(image) \n",
        "       #image =  cv2.GaussianBlur(image,(k_size,k_size),sigma) \n",
        "       # Append to dataset\n",
        "       X_train = np.dstack((X_train,image)) if i>0 else image\n",
        "       y_train = np.append(y_train,label_str)\n",
        "    return X_train,y_train\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk5LIOVt8XIU"
      },
      "source": [
        "# Generate Datasets\n",
        "num_train_examples  = 1500\n",
        "num_test_examples = 500\n",
        "X_train,y_train = gen_img_stack(num_train_examples,rnd=True)\n",
        "X_test,y_test = gen_img_stack(num_test_examples,rnd=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfapVVAdw7yq"
      },
      "source": [
        "3. Process the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jFpS0wTfVt"
      },
      "source": [
        "# Convert the labels to numbers for sklearn.\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "\n",
        "y_train_bof = le.transform(y_train)\n",
        "y_test_bof = le.transform(y_test)\n",
        "\n",
        "list(le.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkDFHd4owukD"
      },
      "source": [
        "4. Plot some examples from the Training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VyzlLQi37Gt"
      },
      "source": [
        "i=0\n",
        "n_plots = 12 # number of plots\n",
        "f, axarr = plt.subplots(1,n_plots,figsize=(12,10))\n",
        "\n",
        "for i in range(n_plots):  # Only take a single example\n",
        "  axarr[i].imshow(X_train[:,:,i],'gray')\n",
        "  axarr[i].axis('off')\n",
        "  #axarr[i].title.set_text(label.numpy()  )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweWe6maFlnC"
      },
      "source": [
        "#B (part 1). Building the Vocabulary\n",
        "The first key step in using a bag of features approach is to build the vocabulary.  We can build a voculabry using the k-means algorithm with $k$ clusters corresponding to unique features. This is an unsupervised algorithm. As detailed in the paper by [Lazebnik](https://inc.ucsd.edu/~marni/Igert/Lazebnik_06.pdf) we can sample features patches (16 x 16) in the form of a dense grid (with spacing every 8 pixels) from the training images. This dense sampling approach works better for scene classification than other approches to finding keypoints (e.g with feature detectors or with random sampling). The patches are converted into SIFT a descriptor vector (using OpenCV) before clustering with the k-means algorithm.\n",
        "\n",
        "Note: Even with a smaller number of training images, using dense sampling creates a very large number of image patches. These patches are converted to a descriptor representation which which must be clustered by the k-means algorithm. The sklearn package contains the [minibatch k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html ) algorithm which can handle the large number of descriptor vectors. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLVU6pF00Y2j"
      },
      "source": [
        "1. Instantiate the SIFT Descriptor,\n",
        "The OpenCV SIFT algorithm can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gClkgKor9fe0"
      },
      "source": [
        "\n",
        "sift = cv2.xfeatures2d.SIFT_create(nfeatures=300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQNNW6IZ0jSy"
      },
      "source": [
        "2. Generate Keypoints.\n",
        "Keypoint locations can be directly supplied to the SIFT algorithm (instead of it finding keypoints in images) This is achieved by directly defining the keypoints `cv2.KeyPoint(x, y, patch_d)` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVLvf3iDTFzS"
      },
      "source": [
        "# Create function to generate keypoints\n",
        "# If all images are the same size then one set of keypoints works for every image\n",
        "# increases speed of code.\n",
        "\n",
        "patch_d = 16 # Diameter of patch\n",
        "step = 8     # keypoint step\n",
        "\n",
        "def fixed_keypoints():\n",
        "    store_keypoints = []\n",
        "    i = 0\n",
        "    x_max = int(np.floor((HW_trg-step)/step)*step) # x stopping value\n",
        "    y_max = int(np.floor((HW_trg-step)/step)*step) # y stopping value\n",
        "    x_pts = np.arange(step-1,x_max,step)  # X keypoints\n",
        "    y_pts = np.arange(step-1,y_max,step)\n",
        "    # Move keypoint across grid\n",
        "    for x in x_pts:\n",
        "      for y in y_pts:\n",
        "        keypoint = cv2.KeyPoint(x, y, patch_d)\n",
        "        store_keypoints.append(keypoint)\n",
        "        i += 1\n",
        "    return store_keypoints        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txcotwn3Z2Qd"
      },
      "source": [
        "# Obtain keypoints for default image\n",
        "keypoints = fixed_keypoints()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlNG9R8X1IFU"
      },
      "source": [
        "3. Obtain array of Descriptors.\n",
        "Using the keypoints we have generated, we can find a set of correponding descriptors in the images of the training set. (Note that we use a sub-set of the training set for speed of execution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIt6nshOsC7R"
      },
      "source": [
        "\n",
        "num_codebook_examples = 650 # Number of training set images to use in building Vocabulary/codebook\n",
        "\n",
        "# Convert part of training set to descriptor representation\n",
        "for i in tqdm(range(num_codebook_examples)):\n",
        "  image = X_train[:,:,i]\n",
        "  # Call a new set of keypoints\n",
        "  keypoints = fixed_keypoints() \n",
        "\n",
        "  # Convert image to sift descriptor\n",
        "  kp1, des1 = sift.compute(image,keypoints,None)\n",
        "  \n",
        "  # store feature descriptors\n",
        "  store_descriptors = np.concatenate((store_descriptors,des1)) if i>0 else des1\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pkQ7vXn1_zs"
      },
      "source": [
        "4. Find Cluster Centres\n",
        "\n",
        "The MiniBatchKMeans algorithm is applied to find cluster centres in the Descriptor data. As with all k-means algorithms, we must set the number of cluster centres `n_c` a priori. Based on the work by Lazebnik `n_c`=200 is a good choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljyDaSnNgGeE"
      },
      "source": [
        "n_c = 200          #Number of cluster centres  \n",
        "batch_size = 5000\n",
        "kmeans = MiniBatchKMeans(n_clusters=n_c,random_state=None,tol=1e-6,batch_size=len(keypoints))\n",
        "# apply descritors to kmeans in mini-batch fashion\n",
        "np.random.shuffle(store_descriptors)\n",
        "num_feature_des = np.shape(store_descriptors)[0]\n",
        "for i in tqdm(range(0,num_feature_des,batch_size)):    \n",
        "  kmeans = kmeans.partial_fit(store_descriptors[i:i+batch_size,:])\n",
        "\n",
        "# Print the cluster centre  \n",
        "kmeans.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWW6zl2N2cis"
      },
      "source": [
        "#B (part2). Visualise the vocabulary.\n",
        "The cluster centres we have obtained in the last step correspond to similar features (or visual words) seen in the training images. \n",
        "\n",
        "In order to visualise these features we will average the a number of image patches corresponding to descriptors that belong to each cluster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry2SwK5h1nm_"
      },
      "source": [
        "# Create an empty code book, that will store the average of images in the cluster.\n",
        "store_code_book = np.zeros([patch_d,patch_d,n_c])\n",
        "code_val_cntr = np.ones([n_c])                #Counts number of iterations of each code centre\n",
        "\n",
        "# Iterate through a limted number of training images\n",
        "n_examples = 200\n",
        "\n",
        "for i in tqdm(range(n_examples)):\n",
        "  # Select image\n",
        "  image = X_train[:,:,i]\n",
        "\n",
        "  # Compute the descriptor from the image\n",
        "  kp1, des1 = sift.compute(image,keypoints,None)\n",
        "\n",
        "  # Predict cluster centres for each descriptor in the image\n",
        "  c_c_val = kmeans.predict(des1)\n",
        "\n",
        "  # Step through image patches corresponding to each descriptor and add to correct average.\n",
        "  for k in range(len(kp1)):\n",
        "     x_pos = int(kp1[k].pt[1])\n",
        "     y_pos = int(kp1[k].pt[0])\n",
        "     image_patch = image[y_pos-step+1:y_pos+step+1,x_pos-step+1:x_pos+step+1]\n",
        "     # Add patch to appropriate code book and update averages (in place).\n",
        "     m = code_val_cntr[c_c_val[k]-1]\n",
        "     store_code_book[0:patch_d,0:patch_d,c_c_val[k]-1] *= m\n",
        "     store_code_book[0:patch_d,0:patch_d,c_c_val[k]-1] += image_patch\n",
        "     store_code_book[0:patch_d,0:patch_d,c_c_val[k]-1] /= m+1\n",
        "     code_val_cntr[c_c_val[k]-1]= m+1\n",
        "\n",
        "# Sort the visual words by number of uses.\n",
        "code_val_arg = np.argsort(code_val_cntr)\n",
        "\n",
        "# Plot averaged image patches corresponding to cluster centres.\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "for i in code_val_arg:#range(np.shape(store_code_book)[2]):\n",
        "     ax = fig.add_subplot(14, 15, i + 1, xticks=[], yticks=[])\n",
        "     ax.imshow(store_code_book[:,:,i],'gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qli6oA_2KW6"
      },
      "source": [
        "#C. Convert to Histogram Representation.\n",
        "1. In order to perform classification, the training and test sets need to be converted to their histogram representations. The descriptors are intially found for the (densely sampeled) image patches in the training images. The cluster to which each descriptor belongs to is then found, using `kmeans.predict`. Finally a histogram is created from the number of occurences of each cluster centre found in each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm-d6wEE2Y2S"
      },
      "source": [
        "# Function to convert training image to \n",
        "def conv_hist_rep(X_train,n_examples):\n",
        "   i=0 \n",
        "   store_hist_vec = np.empty([1,n_c])\n",
        "   label_list = []\n",
        "   for i in tqdm(range(n_examples)):  # Only take a single example\n",
        "      #select image\n",
        "      image = X_train[:,:,i]\n",
        "\n",
        "      # Convert image to sift descriptor\n",
        "      kp1, des1 = sift.compute(image,keypoints,None)\n",
        "      \n",
        "      # Predict cluster centres with descritptors from each image\n",
        "      val = kmeans.predict(des1)\n",
        "      \n",
        "      # Convert to histogram representation \n",
        "      bof_hist,bin_edges = np.histogram(val, bins=n_c, range=(0,n_c-1),density=True)\n",
        "      \n",
        "      bof_hist = np.expand_dims(bof_hist, axis=0)\n",
        "      # Normalise histogram \n",
        "      #bof_hist=bof_hist.astype('float64')\n",
        "      #bof_hist /= np.sum(bof_hist)\n",
        "      # Concatenate to vector (from n_samples,n_vector)\n",
        "      store_hist_vec = np.concatenate((store_hist_vec,bof_hist)) if i>0 else bof_hist\n",
        "      #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "      i=i+1\n",
        "   return store_hist_vec\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq9G0bSSMXnW"
      },
      "source": [
        "X_train_bof=conv_hist_rep(X_train,num_train_examples)\n",
        "X_test_bof=conv_hist_rep(X_test,num_test_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-J228YZI2rl"
      },
      "source": [
        "2. The histograms obtained can be visualised. Ideally histograms for images drawn from the class will have similar distributions, allowing the various classes to be discriminated from one another.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9j34BWzQ53A"
      },
      "source": [
        "ptr=0\n",
        "n_plots = 12 # number of plots\n",
        "f, axarr = plt.subplots(6,2,figsize=(12,10))\n",
        "n_features = np.shape(X_train_bof)[1]\n",
        "\n",
        "for i in range(6):  # Only take a single example\n",
        "  axarr[i,0].imshow(X_test[:,:,i],'gray')\n",
        "  axarr[i,0].axis('off')\n",
        "  axarr[i,1].bar(np.arange(n_features),X_test_bof[i,:])\n",
        "\n",
        "  #axarr[i].title.set_text(label.numpy()  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbztAGYEEKy"
      },
      "source": [
        " # D. (part 1) Training the classifier\n",
        " In order to classify the data, we can train a RBF (Radial Basis Function) Kernel [support vector machine classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) from the sklearn package. The classifier can easily be used once the training data is in the appropriate format of n_samples x n_features, and the labels are in a numerical format. \n",
        "\n",
        " The classifier is trained using the `fit` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zl1i6vKfdcC"
      },
      "source": [
        "svm_clf = svm.SVC(C=100)\n",
        "svm_clf.fit(X_train_bof, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynn0KlP_v7BU"
      },
      "source": [
        "# D. (part 2) Testing the Classifier.\n",
        "We will use the `predict` method from the sklearn svm.SVC class. We will then obtain a classification report to show the performance of our classifier. Some examples  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbAidoT4_fvE"
      },
      "source": [
        "1. Obtain predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I18Lj6EfiCw"
      },
      "source": [
        "y_pred = svm_clf.predict(X_test_bof)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJqse2fH_e_F"
      },
      "source": [
        "2. Print classification report on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odzx-abcfj0_"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test[0:num_test_examples], y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvoMQPzP_dDf"
      },
      "source": [
        "3. Plot some examples from the test dataset with predicted labels. Incorrect labels are marked in red. (As we can see this is a challenging dataset for recognition, as there is often ambiguity and similarity between some scenes. E.g. Mountain & Glacier or Steet & Buildings. However it is observed that the classifier performs reasonable well.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSz1bYwKAinm"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "i=0\n",
        "n_plots = 30\n",
        "for i in range(n_plots):  # Only take a single batch\n",
        "    ax = fig.add_subplot(5, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(X_test[:,:,i],'gray')\n",
        "    ax.axis('off')\n",
        "    color = ('black' if y_pred[i] == y_test[i] else 'red')\n",
        "    ax.set_title(y_pred[i],color=color)\n",
        "    i = i+1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}